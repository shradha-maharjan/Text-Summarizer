{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding to PYTHONPATH: c:\\Users\\FLEX\\Desktop\\Text-Summarizer\\src\n",
      "Contents of src: ['textSummarizer', 'textSummarizer.egg-info']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"src\"))\n",
    "print(\"Adding to PYTHONPATH:\", src_path)\n",
    "print(\"Contents of src:\", os.listdir(src_path))  # should contain 'textSummarizer'\n",
    "\n",
    "sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\FLEX\\\\Desktop\\\\Text-Summarizer\\\\research'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\FLEX\\\\Desktop\\\\Text-Summarizer'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_ckpt: Path\n",
    "    num_train_epochs: int\n",
    "    warmup_steps: int\n",
    "    per_device_train_batch_size: int\n",
    "    weight_decay: float\n",
    "    logging_steps: int\n",
    "    evaluation_strategy: str\n",
    "    eval_steps: int\n",
    "    save_steps: float\n",
    "    gradient_accumulation_steps: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textSummarizer.constants import *\n",
    "from textSummarizer.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TrainingArguments\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            model_ckpt = config.model_ckpt,\n",
    "            num_train_epochs = params.num_train_epochs,\n",
    "            warmup_steps = params.warmup_steps,\n",
    "            per_device_train_batch_size = params.per_device_train_batch_size,\n",
    "            weight_decay = params.weight_decay,\n",
    "            logging_steps = params.logging_steps,\n",
    "            evaluation_strategy = params.evaluation_strategy,\n",
    "            eval_steps = params.evaluation_strategy,\n",
    "            save_steps = params.save_steps,\n",
    "            gradient_accumulation_steps = params.gradient_accumulation_steps\n",
    "        )\n",
    "\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.1-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\flex\\anaconda3\\envs\\textsumm-new\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\flex\\anaconda3\\envs\\textsumm-new\\lib\\site-packages (from torch) (4.14.1)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\flex\\anaconda3\\envs\\textsumm-new\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Downloading torch-2.7.1-cp311-cp311-win_amd64.whl (216.1 MB)\n",
      "   ---------------------------------------- 0.0/216.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/216.1 MB 11.2 MB/s eta 0:00:20\n",
      "   - -------------------------------------- 5.5/216.1 MB 16.8 MB/s eta 0:00:13\n",
      "   - -------------------------------------- 10.0/216.1 MB 18.3 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 14.4/216.1 MB 19.3 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 18.9/216.1 MB 19.9 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 23.9/216.1 MB 20.4 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 28.8/216.1 MB 20.8 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 33.3/216.1 MB 20.9 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 37.5/216.1 MB 20.9 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 41.9/216.1 MB 21.0 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 46.9/216.1 MB 21.2 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 51.1/216.1 MB 21.1 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 55.6/216.1 MB 21.2 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 60.6/216.1 MB 21.3 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 65.3/216.1 MB 21.3 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 70.0/216.1 MB 21.4 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 74.7/216.1 MB 21.5 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 78.9/216.1 MB 21.3 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 82.8/216.1 MB 21.2 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 87.6/216.1 MB 21.2 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 92.5/216.1 MB 21.3 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 97.5/216.1 MB 21.4 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 102.2/216.1 MB 21.5 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 105.9/216.1 MB 21.3 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 109.6/216.1 MB 21.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 113.2/216.1 MB 21.0 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 117.2/216.1 MB 20.8 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 120.8/216.1 MB 20.7 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 124.5/216.1 MB 20.7 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 128.5/216.1 MB 20.6 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 133.2/216.1 MB 20.6 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 137.4/216.1 MB 20.6 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 142.1/216.1 MB 20.6 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 145.5/216.1 MB 20.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 148.9/216.1 MB 20.4 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 152.3/216.1 MB 20.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 157.0/216.1 MB 20.3 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 162.0/216.1 MB 20.4 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 167.2/216.1 MB 20.4 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 171.4/216.1 MB 20.4 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 175.4/216.1 MB 20.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 180.1/216.1 MB 20.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 184.8/216.1 MB 20.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 188.5/216.1 MB 20.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 191.9/216.1 MB 20.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 195.6/216.1 MB 20.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 199.2/216.1 MB 20.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 203.4/216.1 MB 20.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 207.4/216.1 MB 20.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.1/216.1 MB 20.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  216.0/216.1 MB 20.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  216.0/216.1 MB 20.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 216.1/216.1 MB 19.4 MB/s eta 0:00:00\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 2.9/6.3 MB 15.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.3 MB 13.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 10.4 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------  524.3/536.2 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 536.2/536.2 kB 1.3 MB/s eta 0:00:00\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 1.8/2.0 MB 16.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 7.6 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, jinja2, torch\n",
      "\n",
      "   ---------------------------------------- 0/6 [mpmath]\n",
      "   ---------------------------------------- 0/6 [mpmath]\n",
      "   ---------------------------------------- 0/6 [mpmath]\n",
      "   ---------------------------------------- 0/6 [mpmath]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------ --------------------------------- 1/6 [sympy]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   ------------- -------------------------- 2/6 [networkx]\n",
      "   -------------------------- ------------- 4/6 [jinja2]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   --------------------------------- ------ 5/6 [torch]\n",
      "   ---------------------------------------- 6/6 [torch]\n",
      "\n",
      "Successfully installed MarkupSafe-3.0.2 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 sympy-1.14.0 torch-2.7.1\n"
     ]
    }
   ],
   "source": [
    "! pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n",
    "        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt).to(device)\n",
    "        seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)\n",
    "        \n",
    "        #loading data \n",
    "        dataset_samsum_pt = load_from_disk(self.config.data_path)\n",
    "\n",
    "        # trainer_args = TrainingArguments(\n",
    "        #     output_dir=self.config.root_dir, num_train_epochs=self.config.num_train_epochs, warmup_steps=self.config.warmup_steps,\n",
    "        #     per_device_train_batch_size=self.config.per_device_train_batch_size, per_device_eval_batch_size=self.config.per_device_train_batch_size,\n",
    "        #     weight_decay=self.config.weight_decay, logging_steps=self.config.logging_steps,\n",
    "        #     evaluation_strategy=self.config.evaluation_strategy, eval_steps=self.config.eval_steps, save_steps=1e6,\n",
    "        #     gradient_accumulation_steps=self.config.gradient_accumulation_steps\n",
    "        # ) \n",
    "\n",
    "\n",
    "        trainer_args = TrainingArguments(\n",
    "            output_dir=self.config.root_dir, num_train_epochs=1, warmup_steps=500,\n",
    "            per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
    "            weight_decay=0.01, logging_steps=10,\n",
    "            evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n",
    "            gradient_accumulation_steps=16\n",
    "        ) \n",
    "\n",
    "        trainer = Trainer(model=model_pegasus, args=trainer_args,\n",
    "                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
    "                  train_dataset=dataset_samsum_pt[\"train\"], \n",
    "                  eval_dataset=dataset_samsum_pt[\"validation\"])\n",
    "        \n",
    "        trainer.train()\n",
    "\n",
    "        ## Save model\n",
    "        model_pegasus.save_pretrained(os.path.join(self.config.root_dir,\"pegasus-samsum-model\"))\n",
    "        ## Save tokenizer\n",
    "        tokenizer.save_pretrained(os.path.join(self.config.root_dir,\"tokenizer\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-14 17:05:35,756: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-07-14 17:05:35,760: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-07-14 17:05:35,762: INFO: common: created directory at: artifacts]\n",
      "[2025-07-14 17:05:35,764: INFO: common: created directory at: artifacts/model_trainer]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FLEX\\anaconda3\\envs\\textSumm-new\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1737\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1733\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mConverting from Tiktoken\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTikTokenConverter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1735\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1736\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1737\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FLEX\\anaconda3\\envs\\textSumm-new\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1631\u001b[39m, in \u001b[36mTikTokenConverter.converted\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1630\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconverted\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Tokenizer:\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     tokenizer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1632\u001b[39m     tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n\u001b[32m   1633\u001b[39m         [\n\u001b[32m   1634\u001b[39m             pre_tokenizers.Split(Regex(\u001b[38;5;28mself\u001b[39m.pattern), behavior=\u001b[33m\"\u001b[39m\u001b[33misolated\u001b[39m\u001b[33m\"\u001b[39m, invert=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1635\u001b[39m             pre_tokenizers.ByteLevel(add_prefix_space=\u001b[38;5;28mself\u001b[39m.add_prefix_space, use_regex=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1636\u001b[39m         ]\n\u001b[32m   1637\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FLEX\\anaconda3\\envs\\textSumm-new\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1624\u001b[39m, in \u001b[36mTikTokenConverter.tokenizer\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1624\u001b[39m     vocab_scores, merges = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_vocab_merges_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1625\u001b[39m     tokenizer = Tokenizer(BPE(vocab_scores, merges, fuse_unk=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FLEX\\anaconda3\\envs\\textSumm-new\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1600\u001b[39m, in \u001b[36mTikTokenConverter.extract_vocab_merges_from_model\u001b[39m\u001b[34m(self, tiktoken_url)\u001b[39m\n\u001b[32m   1596\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1597\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1598\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1600\u001b[39m bpe_ranks = \u001b[43mload_tiktoken_bpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1601\u001b[39m byte_encoder = bytes_to_unicode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FLEX\\anaconda3\\envs\\textSumm-new\\Lib\\site-packages\\tiktoken\\load.py:148\u001b[39m, in \u001b[36mload_tiktoken_bpe\u001b[39m\u001b[34m(tiktoken_bpe_file, expected_hash)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_tiktoken_bpe\u001b[39m(tiktoken_bpe_file: \u001b[38;5;28mstr\u001b[39m, expected_hash: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m    147\u001b[39m     \u001b[38;5;66;03m# NB: do not add caching to this function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     contents = \u001b[43mread_file_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_bpe_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_hash\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m     ret = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FLEX\\anaconda3\\envs\\textSumm-new\\Lib\\site-packages\\tiktoken\\load.py:48\u001b[39m, in \u001b[36mread_file_cached\u001b[39m\u001b[34m(blobpath, expected_hash)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m read_file(blobpath)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m cache_key = hashlib.sha1(\u001b[43mblobpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m()).hexdigest()\n\u001b[32m     50\u001b[39m cache_path = os.path.join(cache_dir, cache_key)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'encode'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m     model_trainer_config.train()\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m     model_trainer_config = config.get_model_trainer_config()\n\u001b[32m      4\u001b[39m     model_trainer_config = ModelTrainer(config=model_trainer_config)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[43mmodel_trainer_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mModelTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m      8\u001b[39m     device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_ckpt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(\u001b[38;5;28mself\u001b[39m.config.model_ckpt).to(device)\n\u001b[32m     11\u001b[39m     seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FLEX\\anaconda3\\envs\\textSumm-new\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1069\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1066\u001b[39m tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[32m   1068\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1069\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1071\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FLEX\\anaconda3\\envs\\textSumm-new\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2014\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2011\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2012\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2014\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2015\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2016\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2017\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2018\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2019\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2020\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2021\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2022\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2023\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2024\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2025\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2026\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FLEX\\anaconda3\\envs\\textSumm-new\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2260\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2258\u001b[39m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[32m   2259\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2260\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2261\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[32m   2262\u001b[39m     logger.info(\n\u001b[32m   2263\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2264\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2265\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FLEX\\anaconda3\\envs\\textSumm-new\\Lib\\site-packages\\transformers\\models\\pegasus\\tokenization_pegasus_fast.py:136\u001b[39m, in \u001b[36mPegasusTokenizerFast.__init__\u001b[39m\u001b[34m(self, vocab_file, tokenizer_file, pad_token, eos_token, unk_token, mask_token, mask_token_sent, additional_special_tokens, offset, **kwargs)\u001b[39m\n\u001b[32m    132\u001b[39m from_slow = from_slow \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(pad_token) != \u001b[33m\"\u001b[39m\u001b[33m<pad>\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(eos_token) != \u001b[33m\"\u001b[39m\u001b[33m</s>\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(unk_token) != \u001b[33m\"\u001b[39m\u001b[33m<unk>\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33madded_tokens_decoder\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_token_sent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_token_sent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_slow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_slow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28mself\u001b[39m.vocab_file = vocab_file\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FLEX\\anaconda3\\envs\\textSumm-new\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:139\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab_file = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mvocab_file\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    138\u001b[39m     \u001b[38;5;28mself\u001b[39m.additional_special_tokens = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33madditional_special_tokens\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     fast_tokenizer = \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tiktoken\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m     slow_tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FLEX\\anaconda3\\envs\\textSumm-new\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1739\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m TikTokenConverter(\n\u001b[32m   1735\u001b[39m         vocab_file=transformer_tokenizer.vocab_file,\n\u001b[32m   1736\u001b[39m         additional_special_tokens=transformer_tokenizer.additional_special_tokens,\n\u001b[32m   1737\u001b[39m     ).converted()\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1740\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConverting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1741\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwith a SentencePiece tokenizer.model file.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1742\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrently available slow->fast converters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1743\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer_config = ModelTrainer(config=model_trainer_config)\n",
    "    model_trainer_config.train()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textSumm-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
